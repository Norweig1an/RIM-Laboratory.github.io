---
---

@string{aps = {American Physical Society,}}

@article{huang2025twintacwiderangehighlysensitive,
      title={TwinTac: A Wide-Range, Highly Sensitive Tactile Sensor with Real-to-Sim Digital Twin Sensor Model}, 
      author={Xiyan Huang and Zhe Xu and Chenxi Xiao},
      html={https://arxiv.org/abs/2509.10063}, 
      preview={twintac.png},
      selected={true},
      year={2025},
      journal={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
      abbr={IROS 2025},
      abstract={Robot skill acquisition processes driven by reinforcement learning often rely on simulations to efficiently generate large-scale interaction data. However, the absence of simulation models for tactile sensors has hindered the use of tactile sensing in such skill learning processes, limiting the development of effective policies driven by tactile perception. To bridge this gap, we present TwinTac, a system that combines the design of a physical tactile sensor with its digital twin model. Our hardware sensor is designed for high sensitivity and a wide measurement range, enabling high quality sensing data essential for object interaction tasks. Building upon the hardware sensor, we develop the digital twin model using a realto-sim approach. This involves collecting synchronized crossdomain data, including finite element method results and the physical sensor’s outputs, and then training neural networks to map simulated data to real sensor responses. Through experimental evaluation, we characterized the sensitivity of the physical sensor and demonstrated the consistency of the digital twin in replicating the physical sensor’s output. Furthermore, by conducting an object classification task, we showed that simulation data generated by our digital twin sensor can effectively augment real-world data, leading to improved accuracy. These results highlight TwinTac’s potential to bridge the gap in cross-domain learning tasks.}
}

@article{11128300,
  author={Wu, Yifan and Chen, Yuzhou and Zhu, Zhengying and Qin, Xuhao and Xiao, Chenxi},
  journal={2025 IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={HumanFT: A Human-Like Fingertip Multimodal Visuo-Tactile Sensor}, 
  year={2025},
  preview={humanft.png},
  pages={244-250},
  abbr={ICRA 2025},
  selected={true},
  keywords={Vibrations;Hands;Three-dimensional displays;Shape;Force;Tactile sensors;Humanoid robots;Elastomers;Robot sensing systems;Sensors},
  doi={10.1109/ICRA55743.2025.11128300},
  abstract={Tactile sensors play a crucial role in enabling robots to interact effectively and safely with objects in everyday tasks. In particular, visuotactile sensors have seen increasing usage in two and three-fingered grippers due to their highquality feedback. However, a significant gap remains in the development of sensors suitable for humanoid robots, especially f ive-fingered dexterous hands. One reason is because of the challenges in designing and manufacturing sensors that are compact in size. In this paper, we propose HumanFT, a multimodal visuotactile sensor that replicates the shape and functionality of a human fingertip. To bridge the gap between human and robotic tactile sensing, our sensor features real-time force measurements, high-frequency vibration detection, and overtemperature alerts. To achieve this, we developed a suite of fabrication techniques for a new type of elastomer optimized for force propagation and temperature sensing. Besides, our sensor integrates circuits capable of sensing pressure and vibration. These capabilities have been validated through experiments. The proposed design is simple and cost-effective to fabricate. We believe HumanFT can enhance humanoid robots’ perception by capturing and interpreting multimodal tactile information.}
}

@article{lin2025pptacpaperpickingusing,
    title={PP-Tac: Paper Picking Using Tactile Feedback in Dexterous Robotic Hands}, 
    author={Pei Lin and Yuzhe Huang and Wanlin Li and Jianpeng Ma and Chenxi Xiao and Ziyuan Jiao},
    year={2025},
    preview={pptac.png},
    abbr={RSS 2025},
    selected={true},
    journal={2025 Robotics Science and Systems (RSS)}, 
    html={https://arxiv.org/abs/2504.16649}, 
    abstract={Robots are increasingly envisioned as human companions, assisting with everyday tasks that often involve manipulating deformable objects. Recent advancements in robotic hardware and embodied AI algorithms have expanded the range of tasks robots can perform. However, current systems still struggle with handling thin, flat objects like paper and fabric due to limitations in motion planning and perception. This paper introduces PP-Tac, a robotic system designed specifically for handling paper-like objects. We developed a multi-fingered robotic hand equipped with high-resolution tactile sensors that provide omnidirectional feedback, enabling slippage detection and precise friction control with the material. Additionally, we created a grasp trajectory synthesis pipeline to generate a dataset of flat-object grasping motions and trained a diffusion policy for real-time control. This policy was then transferred to a real-world hand-arm platform for extensive evaluation. Our experiments, involving both everyday objects (e.g., plastic bags, paper, cloth) and more challenging materials (e.g., kraft paper handbags), achieved a success rate of 87.5%. By leveraging tactile feedback, our system also adapts to varying surfaces beneath the objects. These results demonstrate the robustness of our approach. We believe PP-Tac has significant potential for applications in household and industrial settings, such as organizing documents, packaging, and cleaning, where precise handling of flat objects is essential.}
}

@article{10943373,
  author={Li, Mujing and Wang, Guanjie and Zhang, Xingguang and Liao, Qifeng and Xiao, Chenxi},
  journal={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)}, 
  title={D-LUT: Photorealistic Style Transfer via Diffusion Process}, 
  year={2025},
  abbr={WACV 2025},
  preview={dlut.png},
  pages={9206-9214},
  keywords={Visualization;Three-dimensional displays;Image color analysis;Heuristic algorithms;Computational modeling;Streaming media;Real-time systems;Table lookup;Computational efficiency;Videos;color transfer;photorealistic style transfer;diffusion process},
  doi={10.1109/WACV61041.2025.00892},
  abstract={Post-editing color in photographs is a crucial process for enhancing a photograph's aesthetic value. Traditionally, this process has required a significant investment of time and manual effort. Previous color transfer algorithms, achieved through encoder-decoder deep learning architectures, have simplified this process. However, these techniques may introduce artifacts and decrease image quality. Moreover, previous approaches are not explainable, making the method less user-friendly. In addition, the computational requirements of these models limit their deployment across various devices. To address these challenges, we introduce the Diffusion-based Look-Up Table (D-LUT). This approach is artifact-free, explainable, computationally efficient, and does not require pretraining stage. It derives a 3D Look-Up Table (3D LUT) for transitioning between the color styles of different images. Specifically, this 3D LUT is obtained using a score-matching algorithm, followed by color distribution alignment through Langevin dynamics. Our proposed D-LUT approach has achieved state-of-the-art performance while requiring significantly less GPU memory than previous baselines. Importantly, the 3D LUTs explicitly derived from the D-LUT algorithm enable color style transfer across broader visual modalities, such as real-time color transfer for videos.}
}

@article{10740799,
  author={Shao, Yanming and Xiao, Chenxi},
  journal={IEEE Robotics and Automation Letters}, 
  title={Bimanual Grasp Synthesis for Dexterous Robot Hands}, 
  year={2024},
  volume={9},
  number={12},
  selected={true},
  preview={bigrasp.png},
  abbr={RA-L 2024},
  pages={11377-11384},
  keywords={Grasping;Robot kinematics;Manipulators;Optimization;Grippers;Computational efficiency;Humanoid robots;Physics;Robot sensing systems;Bimanual manipulation;grasping;dexterous manipulation},
  doi={10.1109/LRA.2024.3490393},
  abstract={Humans naturally perform bimanual skills to handle large and heavy objects. To enhance robots’ object manipulation capabilities, generating effective bimanual grasp poses is essential. Nevertheless, bimanual grasp synthesis for dexterous hand manipulators remains underexplored. To bridge this gap, we propose the BimanGrasp algorithm for synthesizing bimanual grasps on 3D objects. The BimanGrasp algorithm generates grasp poses by optimizing an energy function that considers grasp stability and feasibility. Furthermore, the synthesized grasps are verified using the Isaac Gym physics simulation engine. These verified grasp poses form the BimanGrasp-Dataset, the first largescale synthesized bimanual dexterous hand grasp pose dataset to our knowledge. The dataset comprises over 150k verified grasps on 900 objects, facilitating the synthesis of bimanual grasps through a data-driven approach. Last, we propose BimanGraspDDPM, a diffusion model trained on the BimanGrasp-Dataset. This model achieved a grasp synthesis success rate of 69.87% and significant acceleration in computational speed compared to BimanGrasp algorithm.}
}